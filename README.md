# Collect Evaluation Resources for LLMs

## Evaluation Papers for LLMs
![](https://img.shields.io/badge/Type-Paper-orange)

- AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models-arxiv2023.04 [[paper]](https://arxiv.org/pdf/2304.06364.pdf) [[code]](https://github.com/microsoft/AGIEval)
- Holistic Evaluation of Language Models-arxiv2022.11 [[paper]](https://arxiv.org/abs/2211.09110) [[code]](https://github.com/stanford-crfm/helm) [[platform]](https://crfm.stanford.edu/helm/latest/)
- A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity-arxiv2023.02 [[paper]](https://arxiv.org/abs/2302.04023) [[code]](https://github.com/HLTCHKUST/chatgpt-evaluation)
- Cross-Lingual Summarization via ChatGPT [[paper]](https://arxiv.org/abs/2302.14229)

## Evaluation Tools for LLMs
![](https://img.shields.io/badge/Type-Tool-green)

- ZeroCLUE [[code]](https://github.com/CLUEbenchmark/ZeroCLUE) [[platform]](https://www.cluebenchmarks.com/zeroclue.html)
